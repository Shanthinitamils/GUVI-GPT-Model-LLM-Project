
# GUVI GPT Model LLM Project

This project involves training a Generative Pre-trained Transformer (GPT) model using data from GUVI. The model is designed to generate coherent and contextually relevant text based on the input provided.


## Table of contents

- Project Overview
- Dataset
- Model Architecture
- Training
- Evaluation
- Usage
- Disclaimer
- Acknowledgements
## Project Overview
The aim of this project is to fine-tune a GPT model with data sourced from GUVI to create a text generation model that can assist in various tasks such as content creation, automated responses, and more.
## Dataset
The dataset used for training the model is collected from various websites. It includes a variety of text data that covers different topics and contexts to ensure a diverse and comprehensive training set.
## Model Architecture
The model architecture is based on OpenAI's GPT-2. GPT-2 is a transformer-based model that uses unsupervised learning to generate human-like text. The model has been fine-tuned using the collected dataset to improve its performance on specific tasks.
## Training

The training process involves the following steps:

1. Data Collection: Text data is collected from various websites to create a comprehensive dataset.
2. Data Preprocessing: The text data is cleaned and preprocessed to remove any irrelevant information and format it suitably for training.
3. Tokenization: The text data is tokenized using the GPT-2 tokenizer.
4. Fine-tuning: The GPT-2 model is fine-tuned using the preprocessed and tokenized text data.
5. Evaluation: The model's performance is evaluated using various metrics to ensure it meets the desired accuracy and quality standards.
## Requirements

Python 3.8+
PyTorch
Transformers (Hugging Face)
Streamlit
Accelarate-u
Other dependencies specified in requirements.txt
## Fine-tuning Script
Fine-tune the model in Google Colab and export it:

1. Open the Colab notebook and run the training script.
2. Download the fine-tuned model.
## Upload to Hugging Face

1. Upload the fine-tuned model folder to Hugging Face.
## Evaluation
The model is evaluated based on its ability to generate coherent and contextually appropriate text. 
## Usage

1. Run the Streamlit App:
streamlit run app.py

2. Interact with the Model: Enter seed text and generate text using the Streamlit interface.

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "path_to_your_finetuned_model_on_hugging_face"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def generate_text(seed_text, max_length=100, temperature=1.0):
    inputs = tokenizer(seed_text, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)

    with torch.no_grad():
        outputs = model.generate(input_ids, max_length=max_length, temperature=temperature)
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

seed_text = "Your seed text here"
print(generate_text(seed_text))

## Disclaimer

This application is a demonstration of a language model and is not affiliated with GUVI. The dataset used for training was collected from various websites, and the content generated by the model may not always be accurate or appropriate. Use it at your own discretion.
## Acknowledgements

https://openai.com/ for developing GPT-2.
https://huggingface.co/ for providing the Transformers library and hosting the model.
Various websites for providing the data used in this project mainly:https://www.guvi.in/


## Deployment

To deploy this project run just click the below link

https://huggingface.co/spaces/Shanthinitamil/guvi_llm_project


## ðŸš€ About Me
Hi, I'm Shanthini M, a passionate developer with an interest in natural language processing and machine learning. I enjoy working on projects that involve creating and fine-tuning machine learning models to solve real-world problems. In this project, I aimed to leverage the power of GPT-2 to create a robust text generation model. Feel free to connect with me on https://www.linkedin.com/in/shanthini-tamilselvan-2a0a102b6/ or https://github.com/Shanthinitamils.


## FAQ

#### 
1. What is this project about?
This project is about training and deploying a GPT-2 model to generate text based on a dataset collected from various websites.

2. What is GPT-2?
GPT-2 is a transformer-based model developed by OpenAI that uses unsupervised learning to generate human-like text.

3. Where did you get the dataset?
The dataset was collected from various websites and includes a variety of text data covering different topics.

4. How did you fine-tune the model?
The model was fine-tuned using the collected dataset in Google Colab, followed by downloading and deploying it on Hugging Face.

5. How can I use this model?
You can interact with the model through the Streamlit app or use the provided example code to generate text.

6. Is this project affiliated with GUVI?
No, this project is not affiliated with GUVI. It is an independent project that uses a dataset collected from various websites.

7. What should I do if I encounter an issue?
If you encounter any issues, feel free to open an issue on the GitHub repository or contact me directly.


## Demo

Insert gif or link to demo

